{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aderdouri/PINNs/blob/master/Tutorials/Inverse_pinns_Navier_Stokes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFNtrSNreCAM"
      },
      "source": [
        "# Density and Contour Plots"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the PINN model\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self, input_dim=3, output_dim=3, hidden_layers=8, hidden_units=64):\n",
        "        super(PINN, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, hidden_units))\n",
        "        layers.append(nn.Tanh())  # Activation function\n",
        "\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            layers.append(nn.Linear(hidden_units, hidden_units))\n",
        "            layers.append(nn.Tanh())\n",
        "\n",
        "        layers.append(nn.Linear(hidden_units, output_dim))  # Output: [u, v, p]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Define the Navier-Stokes loss\n",
        "def navier_stokes_loss(model, x, y, t, nu, C1, C2):\n",
        "    inputs = torch.cat([x, y, t], dim=1).requires_grad_(True)\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    u, v, p = outputs[:, 0:1], outputs[:, 1:2], outputs[:, 2:3]\n",
        "\n",
        "    # First-order derivatives\n",
        "    u_t = torch.autograd.grad(u, inputs, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 2:3]\n",
        "    u_x = torch.autograd.grad(u, inputs, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
        "    u_y = torch.autograd.grad(u, inputs, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0][:, 1:2]\n",
        "    v_t = torch.autograd.grad(v, inputs, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 2:3]\n",
        "    v_x = torch.autograd.grad(v, inputs, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
        "    v_y = torch.autograd.grad(v, inputs, grad_outputs=torch.ones_like(v), retain_graph=True, create_graph=True)[0][:, 1:2]\n",
        "    p_x = torch.autograd.grad(p, inputs, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
        "    p_y = torch.autograd.grad(p, inputs, grad_outputs=torch.ones_like(p), retain_graph=True, create_graph=True)[0][:, 1:2]\n",
        "\n",
        "    # Second-order derivatives\n",
        "    u_xx = torch.autograd.grad(u_x, inputs, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
        "    u_yy = torch.autograd.grad(u_y, inputs, grad_outputs=torch.ones_like(u_y), retain_graph=True, create_graph=True)[0][:, 1:1]\n",
        "    v_xx = torch.autograd.grad(v_x, inputs, grad_outputs=torch.ones_like(v_x), retain_graph=True, create_graph=True)[0][:, 0:1]\n",
        "    v_yy = torch.autograd.grad(v_y, inputs, grad_outputs=torch.ones_like(v_y), retain_graph=True, create_graph=True)[0][:, 1:1]\n",
        "\n",
        "    # Continuity equation\n",
        "    continuity = u_x + v_y\n",
        "\n",
        "    # X-momentum equation\n",
        "    x_momentum = (\n",
        "        u_t\n",
        "        + C1 * (u * u_x + v * u_y)\n",
        "        + p_x\n",
        "        - C2 * (u_xx + u_yy)\n",
        "    )\n",
        "\n",
        "    # Y-momentum equation\n",
        "    y_momentum = (\n",
        "        v_t\n",
        "        + C1 * (u * v_x + v * v_y)\n",
        "        + p_y\n",
        "        - C2 * (v_xx + v_yy)\n",
        "    )\n",
        "\n",
        "    # Physics loss\n",
        "    physics_loss = (\n",
        "        torch.mean(continuity**2) +\n",
        "        torch.mean(x_momentum**2) +\n",
        "        torch.mean(y_momentum**2)\n",
        "    )\n",
        "\n",
        "    return physics_loss\n",
        "\n",
        "# Training function\n",
        "def train_pinn(model, x_train, y_train, t_train, u_train, v_train, p_train, epochs=5000, lr=1e-3, nu=0.01, C1=1.0, C2=1.0):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Data loss\n",
        "        inputs = torch.cat([x_train, y_train, t_train], dim=1)\n",
        "        outputs = model(inputs)\n",
        "        u_pred, v_pred, p_pred = outputs[:, 0:1], outputs[:, 1:2], outputs[:, 2:3]\n",
        "        data_loss = (\n",
        "            mse_loss(u_pred, u_train) +\n",
        "            mse_loss(v_pred, v_train) +\n",
        "            mse_loss(p_pred, p_train)\n",
        "        )\n",
        "\n",
        "        # Physics loss\n",
        "        physics_loss = navier_stokes_loss(model, x_train, y_train, t_train, nu, C1, C2)\n",
        "\n",
        "        # Total loss\n",
        "        loss = data_loss + physics_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss.item()}, Data Loss: {data_loss.item()}, Physics Loss: {physics_loss.item()}\")\n",
        "\n",
        "# Example usage\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PINN().to(device)\n",
        "\n",
        "# Assuming x_train, y_train, t_train, u_train, v_train, p_train are preloaded tensors\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "t_train = t_train.to(device)\n",
        "u_train = u_train.to(device)\n",
        "v_train = v_train.to(device)\n",
        "p_train = p_train.to(device)\n",
        "\n",
        "train_pinn(model, x_train, y_train, t_train, u_train, v_train, p_train, epochs=5000, lr=1e-3)\n"
      ],
      "metadata": {
        "id": "ZZHAGNImeJto"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,md"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}