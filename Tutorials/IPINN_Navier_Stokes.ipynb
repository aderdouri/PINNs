{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aderdouri/PINNs/blob/master/Tutorials/IPINN_Navier_Stokes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import scipy.io\n",
        "import torch\n",
        "import io\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "lpWIHXFdqT5Z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_training_data(num):\n",
        "    # Changed to raw URL\n",
        "    url = \"https://github.com/maziarraissi/PINNs/raw/master/main/Data/cylinder_nektar_wake.mat\"\n",
        "    # Step 2: Download the file content\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Ensure the request was successful\n",
        "    # Step 3: Load the .mat file into memory\n",
        "    data = scipy.io.loadmat(io.BytesIO(response.content))\n",
        "\n",
        "    U_star = data[\"U_star\"]  # N x 2 x T\n",
        "    P_star = data[\"p_star\"]  # N x T\n",
        "    t_star = data[\"t\"]  # T x 1\n",
        "    X_star = data[\"X_star\"]  # N x 2\n",
        "    N = X_star.shape[0]\n",
        "    T = t_star.shape[0]\n",
        "    # Rearrange Data\n",
        "    XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n",
        "    YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n",
        "    TT = np.tile(t_star, (1, N)).T  # N x T\n",
        "    UU = U_star[:, 0, :]  # N x T\n",
        "    VV = U_star[:, 1, :]  # N x T\n",
        "    PP = P_star  # N x T\n",
        "    x = XX.flatten()[:, None]  # NT x 1\n",
        "    y = YY.flatten()[:, None]  # NT x 1\n",
        "    t = TT.flatten()[:, None]  # NT x 1\n",
        "    u = UU.flatten()[:, None]  # NT x 1\n",
        "    v = VV.flatten()[:, None]  # NT x 1\n",
        "    p = PP.flatten()[:, None]  # NT x 1\n",
        "    # training domain: X × Y = [1, 8] × [−2, 2] and T = [0, 7]\n",
        "    data1 = np.concatenate([x, y, t, u, v, p], 1)\n",
        "    data2 = data1[:, :][data1[:, 2] <= 7]\n",
        "    data3 = data2[:, :][data2[:, 0] >= 1]\n",
        "    data4 = data3[:, :][data3[:, 0] <= 8]\n",
        "    data5 = data4[:, :][data4[:, 1] >= -2]\n",
        "    data_domain = data5[:, :][data5[:, 1] <= 2]\n",
        "    # choose number of training points: num =7000\n",
        "    idx = np.random.choice(data_domain.shape[0], num, replace=False)\n",
        "    x_train = data_domain[idx, 0:1]\n",
        "    y_train = data_domain[idx, 1:2]\n",
        "    t_train = data_domain[idx, 2:3]\n",
        "    u_train = data_domain[idx, 3:4]\n",
        "    v_train = data_domain[idx, 4:5]\n",
        "    p_train = data_domain[idx, 5:6]\n",
        "    return [x_train, y_train, t_train, u_train, v_train, p_train]"
      ],
      "metadata": {
        "id": "8TP2NS-aqYle"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def loss_function(x, y, t, u_obs, v_obs, p_obs):\n",
        "    C1 = torch.exp(log_C1)\n",
        "    C2 = torch.exp(log_C2)\n",
        "\n",
        "    # Ensure gradients can be computed\n",
        "    t.requires_grad_(True)\n",
        "    x.requires_grad_(True)\n",
        "    y.requires_grad_(True)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(x, y, t)\n",
        "    u_pred, v_pred, p_pred = output[:, 0:1], output[:, 1:2], output[:, 2:3]\n",
        "\n",
        "    # Compute derivatives\n",
        "    u_t = gradients(u_pred, t)\n",
        "    u_x = gradients(u_pred, x)\n",
        "    u_y = gradients(u_pred, y)\n",
        "    u_xx = gradients(u_x, x)\n",
        "    u_yy = gradients(u_y, y)\n",
        "\n",
        "    v_t = gradients(v_pred, t)\n",
        "    v_x = gradients(v_pred, x)\n",
        "    v_y = gradients(v_pred, y)\n",
        "    v_xx = gradients(v_x, x)\n",
        "    v_yy = gradients(v_y, y)\n",
        "\n",
        "    p_x = gradients(p_pred, x)\n",
        "    p_y = gradients(p_pred, y)\n",
        "\n",
        "    # Residuals of Navier-Stokes equations\n",
        "    f_u = u_t + C1 * (u_pred * u_x + v_pred * u_y) + p_x - C2 * (u_xx + u_yy)\n",
        "    f_v = v_t + C1 * (u_pred * v_x + v_pred * v_y) + p_y - C2 * (v_xx + v_yy)\n",
        "\n",
        "    # Continuity equation\n",
        "    f_cont = gradients(u_pred, x) + gradients(v_pred, y)\n",
        "\n",
        "    # Physics-based loss\n",
        "    physics_loss = torch.mean(f_u**2) + torch.mean(f_v**2) + torch.mean(f_cont**2)\n",
        "\n",
        "    # Data loss (mean squared error with observed data)\n",
        "    data_loss = torch.mean((u_pred - u_obs)**2) + \\\n",
        "                torch.mean((v_pred - v_obs)**2) + \\\n",
        "                torch.mean((p_pred - p_obs)**2)\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = physics_loss + data_loss\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "_5Gr5Q8esCfP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function for gradient computation\n",
        "def gradients(y, x):\n",
        "    return torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y), create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "# Define the PINN model\n",
        "class PINN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PINN, self).__init__()\n",
        "        self.hidden_layers = nn.Sequential(\n",
        "            nn.Linear(3, 50),  # Input: (x, y, t)\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(50, 50),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(50, 3)   # Output: (v_x, v_y, p)\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x, y):\n",
        "        inputs = torch.cat([t, x, y], dim=1)\n",
        "        return self.hidden_layers(inputs)\n",
        "\n",
        "# Define the unknown parameters (C1 and C2)\n",
        "log_C1 = torch.tensor([0.0], requires_grad=True, dtype=torch.float32)  # log(C1) for positivity\n",
        "log_C2 = torch.tensor([0.0], requires_grad=True, dtype=torch.float32)  # log(C2) for positivity\n",
        "\n",
        "# Initialize the model\n",
        "model = PINN()"
      ],
      "metadata": {
        "id": "tqWaKj70eyfy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizer for both model parameters and unknown constants\n",
        "optimizer = optim.Adam(list(model.parameters()) + [log_C1, log_C2], lr=0.001)\n",
        "\n",
        "# Training data (replace with actual data)\n",
        "#n_points = 1000\n",
        "#x_train = torch.rand(n_points, 1, requires_grad=True)\n",
        "#y_train = torch.rand(n_points, 1, requires_grad=True)\n",
        "#t_train = torch.rand(n_points, 1, requires_grad=True)\n",
        "#v_train = torch.cat([torch.sin(torch.pi * x_train), torch.cos(torch.pi * y_train)], dim=1)  # Example: true velocities\n",
        "#p_train = torch.zeros_like(x_train)  # Example: true pressure\n",
        "\n",
        "\n",
        "# Training data\n",
        "# Get the training data: num = 7000\n",
        "[x_train, y_train, t_train, u_train, v_train, p_train] = load_training_data(num=7000)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32, requires_grad=True)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32, requires_grad=True)\n",
        "t_train = torch.tensor(t_train, dtype=torch.float32, requires_grad=True)\n",
        "u_train = torch.tensor(u_train, dtype=torch.float32)\n",
        "v_train = torch.tensor(v_train, dtype=torch.float32)\n",
        "p_train = torch.tensor(p_train, dtype=torch.float32)\n",
        "\n",
        "# Training loop\n",
        "n_epochs = 20000\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_function(x_train, y_train, t_train, u_train, v_train, p_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, C1: {torch.exp(log_C1).item():.6f}, C2: {torch.exp(log_C2).item():.6f}\")\n",
        "\n",
        "print(\"Training complete!\")\n",
        "print(f\"Learned C1: {torch.exp(log_C1).item():.6f}\")\n",
        "print(f\"Learned C2: {torch.exp(log_C2).item():.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kw1lHHXqdhs",
        "outputId": "3d3d520e-4e0d-41ef-a45a-5bef48e9f687"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.870026, C1: 1.001001, C2: 0.999000\n",
            "Epoch 100, Loss: 0.158680, C1: 0.953107, C2: 0.935309\n",
            "Epoch 200, Loss: 0.141124, C1: 0.940158, C2: 0.896926\n",
            "Epoch 300, Loss: 0.116116, C1: 0.983790, C2: 0.750978\n",
            "Epoch 400, Loss: 0.100669, C1: 1.075227, C2: 0.624372\n",
            "Epoch 500, Loss: 0.089320, C1: 1.114935, C2: 0.536917\n",
            "Epoch 600, Loss: 0.082386, C1: 1.089481, C2: 0.469762\n",
            "Epoch 700, Loss: 0.075621, C1: 1.058207, C2: 0.415070\n",
            "Epoch 800, Loss: 0.069646, C1: 1.040982, C2: 0.368571\n",
            "Epoch 900, Loss: 0.063844, C1: 1.037469, C2: 0.328288\n",
            "Epoch 1000, Loss: 0.058438, C1: 1.041221, C2: 0.293148\n",
            "Epoch 1100, Loss: 0.053383, C1: 1.044615, C2: 0.262605\n",
            "Epoch 1200, Loss: 0.048628, C1: 1.046888, C2: 0.236032\n",
            "Epoch 1300, Loss: 0.044125, C1: 1.047528, C2: 0.212869\n",
            "Epoch 1400, Loss: 0.040383, C1: 1.044849, C2: 0.192672\n",
            "Epoch 1500, Loss: 0.035882, C1: 1.043405, C2: 0.175101\n",
            "Epoch 1600, Loss: 0.032305, C1: 1.037613, C2: 0.159773\n",
            "Epoch 1700, Loss: 0.029087, C1: 1.034227, C2: 0.146430\n",
            "Epoch 1800, Loss: 0.026267, C1: 1.028516, C2: 0.134770\n",
            "Epoch 1900, Loss: 0.024146, C1: 1.020989, C2: 0.124579\n",
            "Epoch 2000, Loss: 0.021653, C1: 1.018376, C2: 0.115661\n",
            "Epoch 2100, Loss: 0.020288, C1: 1.010532, C2: 0.107805\n",
            "Epoch 2200, Loss: 0.018284, C1: 1.009672, C2: 0.100859\n",
            "Epoch 2300, Loss: 0.016994, C1: 1.005711, C2: 0.094687\n",
            "Epoch 2400, Loss: 0.015563, C1: 1.002549, C2: 0.089170\n",
            "Epoch 2500, Loss: 0.015157, C1: 0.995733, C2: 0.084210\n",
            "Epoch 2600, Loss: 0.013564, C1: 0.997503, C2: 0.079737\n",
            "Epoch 2800, Loss: 0.012008, C1: 0.994130, C2: 0.071975\n",
            "Epoch 2900, Loss: 0.011299, C1: 0.992718, C2: 0.068587\n",
            "Epoch 3000, Loss: 0.010703, C1: 0.989966, C2: 0.065468\n",
            "Epoch 3100, Loss: 0.012250, C1: 0.987759, C2: 0.062593\n",
            "Epoch 3200, Loss: 0.009592, C1: 0.990461, C2: 0.059931\n",
            "Epoch 3300, Loss: 0.011210, C1: 0.983541, C2: 0.057457\n",
            "Epoch 3400, Loss: 0.008693, C1: 0.989545, C2: 0.055157\n",
            "Epoch 3500, Loss: 0.008688, C1: 0.987585, C2: 0.053008\n",
            "Epoch 3600, Loss: 0.007920, C1: 0.989116, C2: 0.051001\n",
            "Epoch 3700, Loss: 0.007592, C1: 0.988640, C2: 0.049121\n",
            "Epoch 3800, Loss: 0.007271, C1: 0.988862, C2: 0.047355\n",
            "Epoch 3900, Loss: 0.006997, C1: 0.988751, C2: 0.045697\n",
            "Epoch 4000, Loss: 0.006740, C1: 0.988286, C2: 0.044136\n",
            "Epoch 4100, Loss: 0.006481, C1: 0.988498, C2: 0.042666\n",
            "Epoch 4200, Loss: 0.006252, C1: 0.988314, C2: 0.041278\n",
            "Epoch 4300, Loss: 0.006883, C1: 0.988392, C2: 0.039964\n",
            "Epoch 4400, Loss: 0.005840, C1: 0.988394, C2: 0.038721\n",
            "Epoch 4500, Loss: 0.005659, C1: 0.988342, C2: 0.037544\n",
            "Epoch 4600, Loss: 0.012032, C1: 0.986682, C2: 0.036427\n",
            "Epoch 4700, Loss: 0.005328, C1: 0.988534, C2: 0.035366\n",
            "Epoch 4800, Loss: 0.005178, C1: 0.988625, C2: 0.034358\n",
            "Epoch 4900, Loss: 0.005087, C1: 0.985367, C2: 0.033398\n",
            "Epoch 5000, Loss: 0.004945, C1: 0.988765, C2: 0.032485\n",
            "Epoch 5100, Loss: 0.004790, C1: 0.988770, C2: 0.031615\n",
            "Epoch 5200, Loss: 0.004769, C1: 0.988193, C2: 0.030786\n",
            "Epoch 5300, Loss: 0.004872, C1: 0.988777, C2: 0.029997\n",
            "Epoch 5400, Loss: 0.004472, C1: 0.988846, C2: 0.029242\n",
            "Epoch 5500, Loss: 0.004394, C1: 0.988627, C2: 0.028522\n",
            "Epoch 5600, Loss: 0.004651, C1: 0.988960, C2: 0.027835\n",
            "Epoch 5700, Loss: 0.004209, C1: 0.988830, C2: 0.027178\n",
            "Epoch 5800, Loss: 0.004136, C1: 0.988554, C2: 0.026549\n",
            "Epoch 5900, Loss: 0.004056, C1: 0.988914, C2: 0.025949\n",
            "Epoch 6000, Loss: 0.003988, C1: 0.988711, C2: 0.025374\n",
            "Epoch 6100, Loss: 0.003925, C1: 0.988636, C2: 0.024823\n",
            "Epoch 6200, Loss: 0.005267, C1: 0.988633, C2: 0.024297\n",
            "Epoch 6300, Loss: 0.003795, C1: 0.988737, C2: 0.023792\n",
            "Epoch 6400, Loss: 0.003739, C1: 0.988709, C2: 0.023307\n",
            "Epoch 6500, Loss: 0.003706, C1: 0.987992, C2: 0.022843\n",
            "Epoch 6600, Loss: 0.003628, C1: 0.988805, C2: 0.022399\n",
            "Epoch 6700, Loss: 0.003594, C1: 0.988649, C2: 0.021972\n",
            "Epoch 6800, Loss: 0.003536, C1: 0.988573, C2: 0.021564\n",
            "Epoch 6900, Loss: 0.006808, C1: 0.988205, C2: 0.021173\n",
            "Epoch 7000, Loss: 0.003437, C1: 0.988792, C2: 0.020796\n",
            "Epoch 7100, Loss: 0.003393, C1: 0.988776, C2: 0.020436\n",
            "Epoch 7200, Loss: 0.003363, C1: 0.988524, C2: 0.020090\n",
            "Epoch 7300, Loss: 0.003351, C1: 0.988979, C2: 0.019760\n",
            "Epoch 7400, Loss: 0.003271, C1: 0.988885, C2: 0.019443\n",
            "Epoch 7500, Loss: 0.003240, C1: 0.988740, C2: 0.019139\n",
            "Epoch 7600, Loss: 0.003315, C1: 0.989073, C2: 0.018848\n",
            "Epoch 7700, Loss: 0.003309, C1: 0.989015, C2: 0.018570\n",
            "Epoch 7800, Loss: 0.003127, C1: 0.989139, C2: 0.018304\n",
            "Epoch 7900, Loss: 0.003122, C1: 0.988655, C2: 0.018048\n",
            "Epoch 8000, Loss: 0.003228, C1: 0.988406, C2: 0.017804\n",
            "Epoch 8100, Loss: 0.003029, C1: 0.989420, C2: 0.017571\n",
            "Epoch 8200, Loss: 0.003001, C1: 0.989241, C2: 0.017348\n",
            "Epoch 8300, Loss: 0.002989, C1: 0.988833, C2: 0.017135\n",
            "Epoch 8400, Loss: 0.002941, C1: 0.989666, C2: 0.016933\n",
            "Epoch 8500, Loss: 0.002914, C1: 0.989604, C2: 0.016739\n",
            "Epoch 8600, Loss: 0.002947, C1: 0.989061, C2: 0.016554\n",
            "Epoch 8700, Loss: 0.005347, C1: 0.989568, C2: 0.016380\n",
            "Epoch 8800, Loss: 0.002832, C1: 0.990010, C2: 0.016212\n",
            "Epoch 8900, Loss: 0.002872, C1: 0.989480, C2: 0.016053\n",
            "Epoch 9000, Loss: 0.002840, C1: 0.989558, C2: 0.015903\n",
            "Epoch 9100, Loss: 0.002785, C1: 0.990352, C2: 0.015760\n",
            "Epoch 9200, Loss: 0.002741, C1: 0.990443, C2: 0.015624\n",
            "Epoch 9300, Loss: 0.003307, C1: 0.990406, C2: 0.015495\n",
            "Epoch 9400, Loss: 0.002687, C1: 0.990558, C2: 0.015373\n",
            "Epoch 9500, Loss: 0.003082, C1: 0.990233, C2: 0.015257\n",
            "Epoch 9600, Loss: 0.002707, C1: 0.990118, C2: 0.015148\n",
            "Epoch 9700, Loss: 0.002642, C1: 0.990616, C2: 0.015044\n",
            "Epoch 9800, Loss: 0.002824, C1: 0.991002, C2: 0.014950\n",
            "Epoch 9900, Loss: 0.002579, C1: 0.991024, C2: 0.014858\n",
            "Epoch 10000, Loss: 0.002814, C1: 0.989862, C2: 0.014771\n",
            "Epoch 10100, Loss: 0.002536, C1: 0.991275, C2: 0.014692\n",
            "Epoch 10200, Loss: 0.002521, C1: 0.991135, C2: 0.014616\n",
            "Epoch 10300, Loss: 0.002659, C1: 0.990641, C2: 0.014544\n",
            "Epoch 10400, Loss: 0.002540, C1: 0.991544, C2: 0.014480\n",
            "Epoch 10500, Loss: 0.002461, C1: 0.991585, C2: 0.014417\n",
            "Epoch 10600, Loss: 0.002569, C1: 0.991040, C2: 0.014356\n",
            "Epoch 10700, Loss: 0.003184, C1: 0.989559, C2: 0.014304\n",
            "Epoch 10800, Loss: 0.002406, C1: 0.991886, C2: 0.014252\n",
            "Epoch 10900, Loss: 0.002448, C1: 0.991931, C2: 0.014202\n",
            "Epoch 11000, Loss: 0.002475, C1: 0.991383, C2: 0.014155\n",
            "Epoch 11100, Loss: 0.002357, C1: 0.991888, C2: 0.014112\n",
            "Epoch 11200, Loss: 0.002394, C1: 0.991216, C2: 0.014071\n",
            "Epoch 11300, Loss: 0.002321, C1: 0.992118, C2: 0.014034\n",
            "Epoch 11400, Loss: 0.002507, C1: 0.989476, C2: 0.013997\n",
            "Epoch 11500, Loss: 0.002288, C1: 0.992441, C2: 0.013961\n",
            "Epoch 11600, Loss: 0.002273, C1: 0.992399, C2: 0.013924\n",
            "Epoch 11700, Loss: 0.002463, C1: 0.990513, C2: 0.013892\n",
            "Epoch 11800, Loss: 0.002242, C1: 0.992649, C2: 0.013862\n",
            "Epoch 11900, Loss: 0.002227, C1: 0.992569, C2: 0.013830\n",
            "Epoch 12000, Loss: 0.002492, C1: 0.989411, C2: 0.013805\n",
            "Epoch 12100, Loss: 0.002197, C1: 0.992814, C2: 0.013777\n",
            "Epoch 12200, Loss: 0.002189, C1: 0.992760, C2: 0.013745\n",
            "Epoch 12300, Loss: 0.002211, C1: 0.992919, C2: 0.013723\n",
            "Epoch 12400, Loss: 0.002155, C1: 0.992949, C2: 0.013701\n",
            "Epoch 12500, Loss: 0.002142, C1: 0.992871, C2: 0.013671\n",
            "Epoch 12600, Loss: 0.002535, C1: 0.993044, C2: 0.013653\n",
            "Epoch 12700, Loss: 0.002114, C1: 0.993116, C2: 0.013631\n",
            "Epoch 12800, Loss: 0.002114, C1: 0.991934, C2: 0.013600\n",
            "Epoch 12900, Loss: 0.002125, C1: 0.993195, C2: 0.013585\n",
            "Epoch 13000, Loss: 0.002074, C1: 0.993244, C2: 0.013564\n",
            "Epoch 13100, Loss: 0.002106, C1: 0.993320, C2: 0.013537\n",
            "Epoch 13200, Loss: 0.002060, C1: 0.993264, C2: 0.013519\n",
            "Epoch 13300, Loss: 0.002036, C1: 0.993372, C2: 0.013502\n",
            "Epoch 13400, Loss: 0.002125, C1: 0.991609, C2: 0.013478\n",
            "Epoch 13500, Loss: 0.002012, C1: 0.993521, C2: 0.013455\n",
            "Epoch 13600, Loss: 0.001998, C1: 0.993446, C2: 0.013432\n",
            "Epoch 13700, Loss: 0.003286, C1: 0.990122, C2: 0.013416\n",
            "Epoch 13800, Loss: 0.001974, C1: 0.993632, C2: 0.013395\n",
            "Epoch 13900, Loss: 0.001969, C1: 0.993495, C2: 0.013365\n",
            "Epoch 14000, Loss: 0.001949, C1: 0.993671, C2: 0.013357\n",
            "Epoch 14100, Loss: 0.001942, C1: 0.993635, C2: 0.013342\n",
            "Epoch 14200, Loss: 0.002472, C1: 0.992265, C2: 0.013311\n",
            "Epoch 14300, Loss: 0.001914, C1: 0.993746, C2: 0.013300\n",
            "Epoch 14400, Loss: 0.001915, C1: 0.993423, C2: 0.013283\n",
            "Epoch 14500, Loss: 0.001890, C1: 0.993873, C2: 0.013257\n",
            "Epoch 14600, Loss: 0.001880, C1: 0.993774, C2: 0.013238\n",
            "Epoch 14700, Loss: 0.002242, C1: 0.991005, C2: 0.013226\n",
            "Epoch 14800, Loss: 0.001856, C1: 0.993953, C2: 0.013207\n",
            "Epoch 14900, Loss: 0.001889, C1: 0.993571, C2: 0.013171\n",
            "Epoch 15000, Loss: 0.001833, C1: 0.993963, C2: 0.013174\n",
            "Epoch 15100, Loss: 0.001825, C1: 0.993953, C2: 0.013156\n",
            "Epoch 15200, Loss: 0.001811, C1: 0.994056, C2: 0.013124\n",
            "Epoch 15300, Loss: 0.001803, C1: 0.993936, C2: 0.013112\n",
            "Epoch 15400, Loss: 0.001801, C1: 0.992750, C2: 0.013097\n",
            "Epoch 15500, Loss: 0.001779, C1: 0.994116, C2: 0.013077\n",
            "Epoch 15600, Loss: 0.001815, C1: 0.992201, C2: 0.013047\n",
            "Epoch 15700, Loss: 0.001945, C1: 0.992982, C2: 0.013050\n",
            "Epoch 15800, Loss: 0.001747, C1: 0.994157, C2: 0.013024\n",
            "Epoch 15900, Loss: 0.001736, C1: 0.994159, C2: 0.012999\n",
            "Epoch 16000, Loss: 0.001726, C1: 0.994115, C2: 0.012976\n",
            "Epoch 16100, Loss: 0.001771, C1: 0.993571, C2: 0.012962\n",
            "Epoch 16200, Loss: 0.002297, C1: 0.994026, C2: 0.012940\n",
            "Epoch 16300, Loss: 0.001700, C1: 0.993256, C2: 0.012925\n",
            "Epoch 16400, Loss: 0.001730, C1: 0.993802, C2: 0.012900\n",
            "Epoch 16500, Loss: 0.001723, C1: 0.993253, C2: 0.012888\n",
            "Epoch 16600, Loss: 0.002186, C1: 0.993042, C2: 0.012863\n",
            "Epoch 16700, Loss: 0.003671, C1: 0.993884, C2: 0.012860\n",
            "Epoch 16800, Loss: 0.001641, C1: 0.994340, C2: 0.012832\n",
            "Epoch 16900, Loss: 0.001632, C1: 0.994323, C2: 0.012823\n",
            "Epoch 17000, Loss: 0.001624, C1: 0.994335, C2: 0.012797\n",
            "Epoch 17100, Loss: 0.001616, C1: 0.994367, C2: 0.012789\n",
            "Epoch 17200, Loss: 0.001634, C1: 0.993524, C2: 0.012759\n",
            "Epoch 17300, Loss: 0.001619, C1: 0.994105, C2: 0.012738\n",
            "Epoch 17400, Loss: 0.001916, C1: 0.994433, C2: 0.012731\n",
            "Epoch 17500, Loss: 0.001573, C1: 0.994429, C2: 0.012710\n",
            "Epoch 17600, Loss: 0.001565, C1: 0.994416, C2: 0.012701\n",
            "Epoch 17700, Loss: 0.001564, C1: 0.993317, C2: 0.012680\n",
            "Epoch 17800, Loss: 0.001547, C1: 0.994505, C2: 0.012658\n",
            "Epoch 17900, Loss: 0.001536, C1: 0.994455, C2: 0.012656\n",
            "Epoch 18000, Loss: 0.001537, C1: 0.994413, C2: 0.012641\n",
            "Epoch 18100, Loss: 0.001686, C1: 0.994514, C2: 0.012610\n",
            "Epoch 18200, Loss: 0.002010, C1: 0.993203, C2: 0.012597\n",
            "Epoch 18300, Loss: 0.001636, C1: 0.993673, C2: 0.012575\n",
            "Epoch 18400, Loss: 0.003217, C1: 0.993627, C2: 0.012573\n",
            "Epoch 18500, Loss: 0.002175, C1: 0.994625, C2: 0.012549\n",
            "Epoch 18600, Loss: 0.001473, C1: 0.994614, C2: 0.012542\n",
            "Epoch 18700, Loss: 0.001464, C1: 0.994611, C2: 0.012527\n",
            "Epoch 18800, Loss: 0.001462, C1: 0.994548, C2: 0.012504\n",
            "Epoch 18900, Loss: 0.001860, C1: 0.992429, C2: 0.012492\n",
            "Epoch 19000, Loss: 0.001439, C1: 0.994707, C2: 0.012475\n",
            "Epoch 19100, Loss: 0.001433, C1: 0.994668, C2: 0.012467\n",
            "Epoch 19200, Loss: 0.001436, C1: 0.992999, C2: 0.012452\n",
            "Epoch 19300, Loss: 0.001414, C1: 0.994756, C2: 0.012437\n",
            "Epoch 19400, Loss: 0.001407, C1: 0.994717, C2: 0.012434\n",
            "Epoch 19500, Loss: 0.001438, C1: 0.994177, C2: 0.012424\n",
            "Epoch 19600, Loss: 0.001390, C1: 0.994821, C2: 0.012395\n",
            "Epoch 19700, Loss: 0.001437, C1: 0.994376, C2: 0.012382\n",
            "Epoch 19800, Loss: 0.001391, C1: 0.993102, C2: 0.012379\n",
            "Epoch 19900, Loss: 0.001367, C1: 0.994868, C2: 0.012363\n",
            "Training complete!\n",
            "Learned C1: 0.994035\n",
            "Learned C2: 0.012350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-1_WW_lqvj3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}