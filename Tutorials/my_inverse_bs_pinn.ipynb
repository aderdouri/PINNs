{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aderdouri/PINNs/blob/master/Tutorials/my_inverse_bs_pinn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKKqc39w7Pnz"
      },
      "source": [
        "# Burgers equation:\n",
        "## Problem setup\n",
        "\n",
        "We will solve a Burgers' equation:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}, \\quad x \\in [-1, 1], \\quad t \\in [0, 1]\n",
        "$$\n",
        "\n",
        "with the Dirichlet boundary conditions and initial conditions:\n",
        "\n",
        "$$\n",
        "u(-1, t) = u(1, t) = 0, \\quad u(x, 0) = -\\sin(\\pi x).\n",
        "$$\n",
        "\n",
        "The reference solution is [Here](https://github.com/lululxvi/deepxde/blob/master/examples/dataset/Burgers.npz)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mz2a6y7tJbdu"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import scipy.io\n",
        "import torch\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Bxo0PgXLuTF",
        "outputId": "81bbd448-1263-4735-d6fb-f46a23ff2784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in the .mat file: dict_keys(['__header__', '__version__', '__globals__', 'x', 't', 'usol'])\n",
            "Shape of u(x, t): torch.Size([256, 100])\n"
          ]
        }
      ],
      "source": [
        "# Step 1: GitHub raw URL to the .mat file\n",
        "url = \"https://github.com/maziarraissi/PINNs/raw/master/appendix/Data/burgers_shock.mat\"\n",
        "\n",
        "# Step 2: Download the file content\n",
        "response = requests.get(url)\n",
        "response.raise_for_status()  # Ensure the request was successful\n",
        "\n",
        "# Step 3: Load the .mat file into memory\n",
        "mat_data = scipy.io.loadmat(io.BytesIO(response.content))\n",
        "\n",
        "# Step 4: Inspect the keys in the .mat file\n",
        "print(\"Keys in the .mat file:\", mat_data.keys())\n",
        "\n",
        "# Step 5: Extract specific variables and convert to PyTorch tensors\n",
        "usol = torch.tensor(mat_data['usol'], dtype=torch.float32)  # Solution variable\n",
        "#x = torch.tensor(mat_data['x'], dtype=torch.float32)        # Spatial grid\n",
        "#t = torch.tensor(mat_data['t'], dtype=torch.float32)        # Time grid\n",
        "\n",
        "# Step 6: Print shapes of the loaded tensors\n",
        "print(f\"Shape of u(x, t): {usol.shape}\")\n",
        "#print(f\"Shape of x: {x.shape}\")\n",
        "#print(f\"Shape of t: {t.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_9U59_Yer5p"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BLAP2KL9b3y8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib.gridspec as gridspec\n",
        "import time\n",
        "\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "class PhysicsInformedNN(nn.Module):\n",
        "    def __init__(self, X, u, layers, lb, ub):\n",
        "        super(PhysicsInformedNN, self).__init__()\n",
        "\n",
        "        self.lb = lb\n",
        "        self.ub = ub\n",
        "\n",
        "        self.x = torch.tensor(X[:,0:1], dtype=torch.float32, requires_grad=True)\n",
        "        self.t = torch.tensor(X[:,1:2], dtype=torch.float32, requires_grad=True)\n",
        "        self.u = torch.tensor(u, dtype=torch.float32)\n",
        "\n",
        "        self.layers = layers\n",
        "\n",
        "        # Initialize NNs\n",
        "        self.model = self.build_model(layers)\n",
        "\n",
        "        # Initialize parameters\n",
        "        self.lambda_1 = nn.Parameter(torch.tensor([0.0], dtype=torch.float32))\n",
        "        self.lambda_2 = nn.Parameter(torch.tensor([-6.0], dtype=torch.float32))\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters())\n",
        "\n",
        "    def build_model(self, layers):\n",
        "        model = nn.Sequential()\n",
        "        num_layers = len(layers)\n",
        "        for i in range(num_layers - 1):\n",
        "            model.add_module(f\"layer_{i}\", nn.Linear(layers[i], layers[i+1]))\n",
        "            if i < num_layers - 2:\n",
        "                model.add_module(f\"tanh_{i}\", nn.Tanh())\n",
        "        return model\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        X = torch.cat([x, t], dim=1)\n",
        "        return self.model(X)\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        x.requires_grad = True\n",
        "        t.requires_grad = True\n",
        "        lambda_1 = self.lambda_1\n",
        "        lambda_2 = torch.exp(self.lambda_2)\n",
        "        u = self.forward(x, t)\n",
        "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
        "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), create_graph=True)[0]\n",
        "        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
        "        return f\n",
        "\n",
        "    def loss_function(self, x, t, u):\n",
        "        u_pred = self.forward(x, t)\n",
        "        f_pred = self.net_f(x, t)\n",
        "        loss = torch.mean((u - u_pred)**2) + torch.mean(f_pred**2)\n",
        "        return loss\n",
        "\n",
        "    def train_model(self, nIter):\n",
        "        start_time = time.time()\n",
        "        for it in range(nIter):\n",
        "            self.optimizer.zero_grad()\n",
        "            loss = self.loss_function(self.x, self.t, self.u)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if it % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                lambda_1_value = self.lambda_1.item()\n",
        "                lambda_2_value = torch.exp(self.lambda_2).item()\n",
        "                print(f'It: {it}, Loss: {loss.item():.3e}, Lambda_1: {lambda_1_value:.3f}, Lambda_2: {lambda_2_value:.6f}, Time: {elapsed:.2f}')\n",
        "                start_time = time.time()\n",
        "\n",
        "        # LBFGS optimizer for fine-tuning\n",
        "        optimizer_lbfgs = torch.optim.LBFGS(model.parameters(),\n",
        "                                            max_iter=50000,\n",
        "                                            tolerance_grad=1e-7,\n",
        "                                            tolerance_change=1e-9)\n",
        "\n",
        "        lbfgs_iter = 0\n",
        "        def closure():\n",
        "            nonlocal lbfgs_iter\n",
        "            optimizer_lbfgs.zero_grad()\n",
        "            loss = self.loss_function(self.x, self.t, self.u)\n",
        "            loss.backward()\n",
        "\n",
        "            # Log progress\n",
        "            if lbfgs_iter % 100 == 0:\n",
        "                lambda_1_value = self.lambda_1.item()\n",
        "                lambda_2_value = torch.exp(self.lambda_2).item()\n",
        "                print(f\"LBFGS Iteration {lbfgs_iter}, Loss: {loss.item():.6f}, lambda_1: {lambda_1_value:.6f}, lambda_2: {lambda_2_value:.6f}\")\n",
        "            lbfgs_iter += 1\n",
        "\n",
        "            return loss\n",
        "\n",
        "        print(\"Starting LBFGS optimization...\")\n",
        "        optimizer_lbfgs.step(closure)\n",
        "\n",
        "    def predict(self, X_star):\n",
        "        x_star = torch.tensor(X_star[:,0:1], dtype=torch.float32)\n",
        "        t_star = torch.tensor(X_star[:,1:2], dtype=torch.float32)\n",
        "        u_star = self.forward(x_star, t_star).detach().numpy()\n",
        "        f_star = self.net_f(x_star, t_star).detach().numpy()\n",
        "        return u_star, f_star"
      ],
      "metadata": {
        "id": "ycwSorGDIIOG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nu = 0.01/np.pi\n",
        "\n",
        "N_u = 2000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "data = mat_data\n",
        "\n",
        "t = data['t'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = np.real(data['usol']).T\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact.flatten()[:,None]\n",
        "\n",
        "# Domain bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "######################################################################\n",
        "######################## Noiseless Data ##############################\n",
        "######################################################################\n",
        "noise = 0.0\n",
        "\n",
        "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "X_u_train = X_star[idx,:]\n",
        "u_train = u_star[idx,:]\n",
        "\n",
        "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "model.train_model(10000)\n",
        "\n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "\n",
        "lambda_1_value = model.lambda_1.item()\n",
        "lambda_2_value = torch.exp(model.lambda_2).item()\n",
        "\n",
        "error_lambda_1 = np.abs(lambda_1_value - 1.0)*100\n",
        "error_lambda_2 = np.abs(lambda_2_value - nu)/nu * 100\n",
        "\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error l1: %.5f%%' % (error_lambda_1))\n",
        "print('Error l2: %.5f%%' % (error_lambda_2))\n",
        "\n",
        "######################################################################\n",
        "########################### Noisy Data ###############################\n",
        "######################################################################\n",
        "noise = 0.01\n",
        "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
        "\n",
        "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
        "model.train_model(10000)\n",
        "\n",
        "u_pred, f_pred = model.predict(X_star)\n",
        "\n",
        "lambda_1_value_noisy = model.lambda_1.item()\n",
        "lambda_2_value_noisy = torch.exp(model.lambda_2).item()\n",
        "\n",
        "error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0)*100\n",
        "error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu)/nu * 100\n",
        "\n",
        "print('Error lambda_1: %f%%' % (error_lambda_1_noisy))\n",
        "print('Error lambda_2: %f%%' % (error_lambda_2_noisy))"
      ],
      "metadata": {
        "id": "pFHFmKrr0gL5",
        "outputId": "20c96b03-8493-473b-cb25-1b2460e20f76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It: 0, Loss: 5.697e-01, Lambda_1: 0.001, Lambda_2: 0.002478, Time: 0.08\n",
            "It: 100, Loss: 1.089e-01, Lambda_1: -0.014, Lambda_2: 0.002609, Time: 2.89\n",
            "It: 200, Loss: 3.343e-02, Lambda_1: 0.056, Lambda_2: 0.002298, Time: 3.51\n",
            "It: 300, Loss: 2.855e-02, Lambda_1: 0.088, Lambda_2: 0.002308, Time: 3.56\n",
            "It: 400, Loss: 2.509e-02, Lambda_1: 0.136, Lambda_2: 0.002290, Time: 2.84\n",
            "It: 500, Loss: 2.320e-02, Lambda_1: 0.173, Lambda_2: 0.002333, Time: 2.88\n",
            "It: 600, Loss: 2.245e-02, Lambda_1: 0.195, Lambda_2: 0.002453, Time: 2.98\n",
            "It: 700, Loss: 2.148e-02, Lambda_1: 0.221, Lambda_2: 0.002597, Time: 4.12\n",
            "It: 800, Loss: 1.995e-02, Lambda_1: 0.257, Lambda_2: 0.002750, Time: 2.84\n",
            "It: 900, Loss: 1.729e-02, Lambda_1: 0.310, Lambda_2: 0.002886, Time: 2.90\n",
            "It: 1000, Loss: 1.536e-02, Lambda_1: 0.378, Lambda_2: 0.003043, Time: 2.87\n",
            "It: 1100, Loss: 1.255e-02, Lambda_1: 0.433, Lambda_2: 0.003233, Time: 3.78\n",
            "It: 1200, Loss: 1.049e-02, Lambda_1: 0.479, Lambda_2: 0.003369, Time: 3.26\n",
            "It: 1300, Loss: 9.473e-03, Lambda_1: 0.513, Lambda_2: 0.003449, Time: 2.84\n",
            "It: 1400, Loss: 8.839e-03, Lambda_1: 0.546, Lambda_2: 0.003511, Time: 2.84\n",
            "It: 1500, Loss: 8.507e-03, Lambda_1: 0.567, Lambda_2: 0.003562, Time: 3.20\n",
            "It: 1600, Loss: 7.707e-03, Lambda_1: 0.582, Lambda_2: 0.003609, Time: 3.81\n",
            "It: 1700, Loss: 7.187e-03, Lambda_1: 0.601, Lambda_2: 0.003656, Time: 2.82\n",
            "It: 1800, Loss: 6.652e-03, Lambda_1: 0.611, Lambda_2: 0.003694, Time: 2.91\n",
            "It: 1900, Loss: 6.509e-03, Lambda_1: 0.627, Lambda_2: 0.003729, Time: 2.89\n",
            "It: 2000, Loss: 6.037e-03, Lambda_1: 0.642, Lambda_2: 0.003773, Time: 3.91\n",
            "It: 2100, Loss: 1.150e-02, Lambda_1: 0.614, Lambda_2: 0.003762, Time: 3.48\n",
            "It: 2200, Loss: 5.988e-03, Lambda_1: 0.607, Lambda_2: 0.003811, Time: 2.80\n",
            "It: 2300, Loss: 5.475e-03, Lambda_1: 0.626, Lambda_2: 0.003824, Time: 2.84\n",
            "It: 2400, Loss: 5.104e-03, Lambda_1: 0.644, Lambda_2: 0.003839, Time: 3.46\n",
            "It: 2500, Loss: 4.773e-03, Lambda_1: 0.659, Lambda_2: 0.003859, Time: 3.52\n",
            "It: 2600, Loss: 4.459e-03, Lambda_1: 0.674, Lambda_2: 0.003880, Time: 3.17\n",
            "It: 2700, Loss: 4.160e-03, Lambda_1: 0.688, Lambda_2: 0.003903, Time: 2.83\n",
            "It: 2800, Loss: 3.878e-03, Lambda_1: 0.702, Lambda_2: 0.003928, Time: 3.00\n",
            "It: 2900, Loss: 4.949e-03, Lambda_1: 0.716, Lambda_2: 0.003952, Time: 3.97\n",
            "It: 3000, Loss: 3.418e-03, Lambda_1: 0.724, Lambda_2: 0.003974, Time: 2.91\n",
            "It: 3100, Loss: 7.444e-03, Lambda_1: 0.736, Lambda_2: 0.003994, Time: 2.84\n",
            "It: 3200, Loss: 4.713e-03, Lambda_1: 0.680, Lambda_2: 0.004042, Time: 2.85\n",
            "It: 3300, Loss: 3.682e-03, Lambda_1: 0.692, Lambda_2: 0.004054, Time: 3.76\n",
            "It: 3400, Loss: 3.369e-03, Lambda_1: 0.708, Lambda_2: 0.004062, Time: 3.23\n",
            "It: 3500, Loss: 3.146e-03, Lambda_1: 0.723, Lambda_2: 0.004073, Time: 2.84\n",
            "It: 3600, Loss: 2.959e-03, Lambda_1: 0.737, Lambda_2: 0.004086, Time: 2.83\n",
            "It: 3700, Loss: 2.794e-03, Lambda_1: 0.749, Lambda_2: 0.004101, Time: 3.27\n",
            "It: 3800, Loss: 2.646e-03, Lambda_1: 0.761, Lambda_2: 0.004116, Time: 3.81\n",
            "It: 3900, Loss: 2.508e-03, Lambda_1: 0.771, Lambda_2: 0.004131, Time: 2.85\n",
            "It: 4000, Loss: 2.379e-03, Lambda_1: 0.782, Lambda_2: 0.004147, Time: 2.87\n",
            "It: 4100, Loss: 2.256e-03, Lambda_1: 0.791, Lambda_2: 0.004164, Time: 2.86\n",
            "It: 4200, Loss: 2.136e-03, Lambda_1: 0.800, Lambda_2: 0.004181, Time: 4.01\n",
            "It: 4300, Loss: 2.019e-03, Lambda_1: 0.809, Lambda_2: 0.004198, Time: 2.96\n",
            "It: 4400, Loss: 8.687e-03, Lambda_1: 0.752, Lambda_2: 0.004219, Time: 2.89\n",
            "It: 4500, Loss: 2.854e-03, Lambda_1: 0.737, Lambda_2: 0.004275, Time: 2.83\n",
            "It: 4600, Loss: 2.451e-03, Lambda_1: 0.754, Lambda_2: 0.004276, Time: 3.41\n",
            "It: 4700, Loss: 2.242e-03, Lambda_1: 0.770, Lambda_2: 0.004281, Time: 3.61\n",
            "It: 4800, Loss: 2.089e-03, Lambda_1: 0.783, Lambda_2: 0.004289, Time: 2.89\n",
            "It: 4900, Loss: 1.961e-03, Lambda_1: 0.796, Lambda_2: 0.004299, Time: 2.88\n",
            "It: 5000, Loss: 1.849e-03, Lambda_1: 0.807, Lambda_2: 0.004310, Time: 2.87\n",
            "It: 5100, Loss: 1.748e-03, Lambda_1: 0.817, Lambda_2: 0.004320, Time: 4.12\n",
            "It: 5200, Loss: 1.656e-03, Lambda_1: 0.826, Lambda_2: 0.004330, Time: 2.82\n",
            "It: 5300, Loss: 1.569e-03, Lambda_1: 0.835, Lambda_2: 0.004341, Time: 2.84\n",
            "It: 5400, Loss: 1.488e-03, Lambda_1: 0.843, Lambda_2: 0.004351, Time: 2.84\n",
            "It: 5500, Loss: 1.410e-03, Lambda_1: 0.851, Lambda_2: 0.004360, Time: 4.09\n",
            "It: 5600, Loss: 1.336e-03, Lambda_1: 0.858, Lambda_2: 0.004369, Time: 3.27\n",
            "It: 5700, Loss: 1.953e-03, Lambda_1: 0.865, Lambda_2: 0.004377, Time: 2.88\n",
            "It: 5800, Loss: 1.367e-03, Lambda_1: 0.871, Lambda_2: 0.004384, Time: 2.84\n",
            "It: 5900, Loss: 1.171e-03, Lambda_1: 0.876, Lambda_2: 0.004389, Time: 3.27\n",
            "It: 6000, Loss: 1.380e-03, Lambda_1: 0.881, Lambda_2: 0.004394, Time: 4.79\n",
            "It: 6100, Loss: 1.132e-03, Lambda_1: 0.885, Lambda_2: 0.004397, Time: 2.91\n",
            "It: 6200, Loss: 1.263e-03, Lambda_1: 0.888, Lambda_2: 0.004399, Time: 2.90\n",
            "It: 6300, Loss: 1.017e-03, Lambda_1: 0.892, Lambda_2: 0.004402, Time: 2.95\n",
            "It: 6400, Loss: 8.069e-03, Lambda_1: 0.850, Lambda_2: 0.004408, Time: 4.07\n",
            "It: 6500, Loss: 1.635e-03, Lambda_1: 0.828, Lambda_2: 0.004471, Time: 2.88\n",
            "It: 6600, Loss: 1.344e-03, Lambda_1: 0.836, Lambda_2: 0.004470, Time: 2.87\n",
            "It: 6700, Loss: 1.221e-03, Lambda_1: 0.846, Lambda_2: 0.004469, Time: 2.86\n",
            "It: 6800, Loss: 1.132e-03, Lambda_1: 0.855, Lambda_2: 0.004470, Time: 4.60\n",
            "It: 6900, Loss: 1.059e-03, Lambda_1: 0.863, Lambda_2: 0.004471, Time: 6.40\n",
            "It: 7000, Loss: 9.970e-04, Lambda_1: 0.871, Lambda_2: 0.004472, Time: 3.67\n",
            "It: 7100, Loss: 9.430e-04, Lambda_1: 0.878, Lambda_2: 0.004473, Time: 3.33\n",
            "It: 7200, Loss: 8.955e-04, Lambda_1: 0.885, Lambda_2: 0.004475, Time: 3.73\n",
            "It: 7300, Loss: 8.532e-04, Lambda_1: 0.891, Lambda_2: 0.004476, Time: 3.02\n",
            "It: 7400, Loss: 8.149e-04, Lambda_1: 0.897, Lambda_2: 0.004477, Time: 2.85\n",
            "It: 7500, Loss: 7.799e-04, Lambda_1: 0.903, Lambda_2: 0.004477, Time: 2.82\n",
            "It: 7600, Loss: 7.474e-04, Lambda_1: 0.908, Lambda_2: 0.004478, Time: 4.08\n",
            "It: 7700, Loss: 7.170e-04, Lambda_1: 0.913, Lambda_2: 0.004478, Time: 2.92\n",
            "It: 7800, Loss: 6.882e-04, Lambda_1: 0.917, Lambda_2: 0.004477, Time: 2.86\n",
            "It: 7900, Loss: 6.608e-04, Lambda_1: 0.921, Lambda_2: 0.004477, Time: 2.86\n",
            "It: 8000, Loss: 6.344e-04, Lambda_1: 0.925, Lambda_2: 0.004476, Time: 3.46\n",
            "It: 8100, Loss: 6.368e-04, Lambda_1: 0.928, Lambda_2: 0.004474, Time: 3.55\n",
            "It: 8200, Loss: 6.485e-04, Lambda_1: 0.931, Lambda_2: 0.004472, Time: 2.87\n",
            "It: 8300, Loss: 7.524e-04, Lambda_1: 0.933, Lambda_2: 0.004471, Time: 2.88\n",
            "It: 8400, Loss: 3.075e-03, Lambda_1: 0.934, Lambda_2: 0.004466, Time: 2.89\n",
            "It: 8500, Loss: 9.132e-03, Lambda_1: 0.872, Lambda_2: 0.004496, Time: 4.13\n",
            "It: 8600, Loss: 2.159e-03, Lambda_1: 0.848, Lambda_2: 0.004556, Time: 3.17\n",
            "It: 8700, Loss: 1.415e-03, Lambda_1: 0.851, Lambda_2: 0.004558, Time: 2.86\n",
            "It: 8800, Loss: 1.175e-03, Lambda_1: 0.857, Lambda_2: 0.004553, Time: 2.83\n",
            "It: 8900, Loss: 1.038e-03, Lambda_1: 0.863, Lambda_2: 0.004549, Time: 3.77\n",
            "It: 9000, Loss: 9.418e-04, Lambda_1: 0.870, Lambda_2: 0.004546, Time: 3.20\n",
            "It: 9100, Loss: 8.673e-04, Lambda_1: 0.876, Lambda_2: 0.004543, Time: 3.11\n",
            "It: 9200, Loss: 8.060e-04, Lambda_1: 0.882, Lambda_2: 0.004541, Time: 2.89\n",
            "It: 9300, Loss: 7.535e-04, Lambda_1: 0.888, Lambda_2: 0.004539, Time: 3.39\n",
            "It: 9400, Loss: 7.078e-04, Lambda_1: 0.894, Lambda_2: 0.004538, Time: 3.63\n",
            "It: 9500, Loss: 6.674e-04, Lambda_1: 0.900, Lambda_2: 0.004536, Time: 2.84\n",
            "It: 9600, Loss: 6.316e-04, Lambda_1: 0.905, Lambda_2: 0.004534, Time: 2.85\n",
            "It: 9700, Loss: 5.996e-04, Lambda_1: 0.910, Lambda_2: 0.004532, Time: 2.84\n",
            "It: 9800, Loss: 5.708e-04, Lambda_1: 0.915, Lambda_2: 0.004530, Time: 4.15\n",
            "It: 9900, Loss: 5.447e-04, Lambda_1: 0.920, Lambda_2: 0.004527, Time: 2.86\n",
            "Starting LBFGS optimization...\n",
            "LBFGS Iteration 0, Loss: 0.000521, lambda_1: 0.924390, lambda_2: 0.004525\n",
            "LBFGS Iteration 100, Loss: 0.000242, lambda_1: 0.974979, lambda_2: 0.004235\n",
            "LBFGS Iteration 200, Loss: 0.000124, lambda_1: 0.988977, lambda_2: 0.003799\n",
            "LBFGS Iteration 300, Loss: 0.000078, lambda_1: 0.992962, lambda_2: 0.003572\n",
            "LBFGS Iteration 400, Loss: 0.000052, lambda_1: 0.995399, lambda_2: 0.003349\n",
            "LBFGS Iteration 500, Loss: 0.000035, lambda_1: 0.996254, lambda_2: 0.003192\n",
            "LBFGS Iteration 600, Loss: 0.000027, lambda_1: 0.998140, lambda_2: 0.003120\n",
            "LBFGS Iteration 700, Loss: 0.000022, lambda_1: 0.996612, lambda_2: 0.003105\n",
            "LBFGS Iteration 800, Loss: 0.000018, lambda_1: 0.998751, lambda_2: 0.003141\n",
            "Error u: 3.176827e-03\n",
            "Error l1: 0.10246%\n",
            "Error l2: 0.43738%\n",
            "It: 0, Loss: 5.569e-01, Lambda_1: -0.001, Lambda_2: 0.002480, Time: 0.04\n",
            "It: 100, Loss: 9.759e-02, Lambda_1: 0.090, Lambda_2: 0.002234, Time: 2.84\n",
            "It: 200, Loss: 3.508e-02, Lambda_1: 0.095, Lambda_2: 0.002107, Time: 2.87\n",
            "It: 300, Loss: 2.679e-02, Lambda_1: 0.115, Lambda_2: 0.002143, Time: 2.85\n",
            "It: 400, Loss: 2.352e-02, Lambda_1: 0.153, Lambda_2: 0.002140, Time: 3.92\n",
            "It: 500, Loss: 2.167e-02, Lambda_1: 0.197, Lambda_2: 0.002234, Time: 3.13\n",
            "It: 600, Loss: 1.946e-02, Lambda_1: 0.241, Lambda_2: 0.002377, Time: 2.87\n",
            "It: 700, Loss: 1.583e-02, Lambda_1: 0.310, Lambda_2: 0.002522, Time: 2.85\n",
            "It: 800, Loss: 1.361e-02, Lambda_1: 0.379, Lambda_2: 0.002667, Time: 3.30\n",
            "It: 900, Loss: 1.141e-02, Lambda_1: 0.437, Lambda_2: 0.002781, Time: 3.66\n",
            "It: 1000, Loss: 1.209e-01, Lambda_1: 0.477, Lambda_2: 0.002797, Time: 2.83\n",
            "It: 1100, Loss: 1.094e-02, Lambda_1: 0.436, Lambda_2: 0.002832, Time: 2.88\n",
            "It: 1200, Loss: 1.004e-02, Lambda_1: 0.467, Lambda_2: 0.002847, Time: 2.92\n",
            "It: 1300, Loss: 9.356e-03, Lambda_1: 0.493, Lambda_2: 0.002868, Time: 4.13\n",
            "It: 1400, Loss: 8.765e-03, Lambda_1: 0.518, Lambda_2: 0.002894, Time: 2.87\n",
            "It: 1500, Loss: 8.244e-03, Lambda_1: 0.540, Lambda_2: 0.002926, Time: 2.85\n",
            "It: 1600, Loss: 7.763e-03, Lambda_1: 0.561, Lambda_2: 0.002965, Time: 2.87\n",
            "It: 1700, Loss: 7.305e-03, Lambda_1: 0.581, Lambda_2: 0.003010, Time: 3.55\n",
            "It: 1800, Loss: 1.632e-02, Lambda_1: 0.594, Lambda_2: 0.003044, Time: 3.44\n",
            "It: 1900, Loss: 6.803e-03, Lambda_1: 0.592, Lambda_2: 0.003084, Time: 2.85\n",
            "It: 2000, Loss: 6.447e-03, Lambda_1: 0.608, Lambda_2: 0.003110, Time: 2.84\n",
            "It: 2100, Loss: 6.100e-03, Lambda_1: 0.624, Lambda_2: 0.003137, Time: 3.02\n",
            "It: 2200, Loss: 5.757e-03, Lambda_1: 0.639, Lambda_2: 0.003165, Time: 4.02\n",
            "It: 2300, Loss: 6.228e-03, Lambda_1: 0.613, Lambda_2: 0.003191, Time: 2.86\n",
            "It: 2400, Loss: 5.756e-03, Lambda_1: 0.624, Lambda_2: 0.003205, Time: 2.87\n",
            "It: 2500, Loss: 5.498e-03, Lambda_1: 0.637, Lambda_2: 0.003218, Time: 2.86\n",
            "It: 2600, Loss: 5.265e-03, Lambda_1: 0.650, Lambda_2: 0.003233, Time: 3.71\n",
            "It: 2700, Loss: 5.045e-03, Lambda_1: 0.662, Lambda_2: 0.003248, Time: 3.23\n",
            "It: 2800, Loss: 4.832e-03, Lambda_1: 0.674, Lambda_2: 0.003263, Time: 2.85\n",
            "It: 2900, Loss: 4.627e-03, Lambda_1: 0.685, Lambda_2: 0.003278, Time: 2.86\n",
            "It: 3000, Loss: 2.391e-02, Lambda_1: 0.665, Lambda_2: 0.003273, Time: 3.22\n",
            "It: 3100, Loss: 5.734e-03, Lambda_1: 0.627, Lambda_2: 0.003347, Time: 3.82\n",
            "It: 3200, Loss: 5.076e-03, Lambda_1: 0.638, Lambda_2: 0.003358, Time: 2.82\n",
            "It: 3300, Loss: 4.822e-03, Lambda_1: 0.651, Lambda_2: 0.003370, Time: 2.82\n",
            "It: 3400, Loss: 4.636e-03, Lambda_1: 0.663, Lambda_2: 0.003382, Time: 2.87\n",
            "It: 3500, Loss: 4.477e-03, Lambda_1: 0.674, Lambda_2: 0.003394, Time: 3.89\n",
            "It: 3600, Loss: 4.333e-03, Lambda_1: 0.684, Lambda_2: 0.003407, Time: 3.09\n",
            "It: 3700, Loss: 4.197e-03, Lambda_1: 0.694, Lambda_2: 0.003420, Time: 2.85\n",
            "It: 3800, Loss: 4.067e-03, Lambda_1: 0.703, Lambda_2: 0.003433, Time: 2.84\n",
            "It: 3900, Loss: 3.939e-03, Lambda_1: 0.712, Lambda_2: 0.003446, Time: 3.30\n",
            "It: 4000, Loss: 3.813e-03, Lambda_1: 0.720, Lambda_2: 0.003459, Time: 3.76\n",
            "It: 4100, Loss: 3.686e-03, Lambda_1: 0.728, Lambda_2: 0.003473, Time: 2.85\n",
            "It: 4200, Loss: 3.556e-03, Lambda_1: 0.735, Lambda_2: 0.003486, Time: 2.82\n",
            "It: 4300, Loss: 3.424e-03, Lambda_1: 0.743, Lambda_2: 0.003499, Time: 3.41\n",
            "It: 4400, Loss: 3.638e-03, Lambda_1: 0.750, Lambda_2: 0.003511, Time: 4.06\n",
            "It: 4500, Loss: 3.161e-03, Lambda_1: 0.756, Lambda_2: 0.003524, Time: 2.84\n",
            "It: 4600, Loss: 3.116e-03, Lambda_1: 0.761, Lambda_2: 0.003536, Time: 2.87\n",
            "It: 4700, Loss: 3.146e-03, Lambda_1: 0.767, Lambda_2: 0.003545, Time: 2.93\n",
            "It: 4800, Loss: 4.704e-03, Lambda_1: 0.747, Lambda_2: 0.003575, Time: 3.80\n",
            "It: 4900, Loss: 2.969e-03, Lambda_1: 0.745, Lambda_2: 0.003595, Time: 3.30\n",
            "It: 5000, Loss: 2.843e-03, Lambda_1: 0.754, Lambda_2: 0.003604, Time: 2.88\n",
            "It: 5100, Loss: 2.731e-03, Lambda_1: 0.762, Lambda_2: 0.003611, Time: 2.83\n",
            "It: 5200, Loss: 2.621e-03, Lambda_1: 0.771, Lambda_2: 0.003618, Time: 3.16\n",
            "It: 5300, Loss: 2.513e-03, Lambda_1: 0.779, Lambda_2: 0.003625, Time: 3.82\n",
            "It: 5400, Loss: 2.406e-03, Lambda_1: 0.787, Lambda_2: 0.003631, Time: 2.84\n",
            "It: 5500, Loss: 2.299e-03, Lambda_1: 0.794, Lambda_2: 0.003637, Time: 2.83\n",
            "It: 5600, Loss: 5.009e-03, Lambda_1: 0.801, Lambda_2: 0.003640, Time: 2.89\n",
            "It: 5700, Loss: 5.421e-03, Lambda_1: 0.716, Lambda_2: 0.003728, Time: 4.00\n",
            "It: 5800, Loss: 3.217e-03, Lambda_1: 0.719, Lambda_2: 0.003746, Time: 3.02\n",
            "It: 5900, Loss: 2.866e-03, Lambda_1: 0.731, Lambda_2: 0.003756, Time: 2.88\n",
            "It: 6000, Loss: 2.676e-03, Lambda_1: 0.743, Lambda_2: 0.003766, Time: 2.81\n",
            "It: 6100, Loss: 2.529e-03, Lambda_1: 0.754, Lambda_2: 0.003775, Time: 3.29\n",
            "It: 6200, Loss: 2.402e-03, Lambda_1: 0.765, Lambda_2: 0.003783, Time: 3.61\n",
            "It: 6300, Loss: 2.287e-03, Lambda_1: 0.775, Lambda_2: 0.003792, Time: 2.87\n",
            "It: 6400, Loss: 2.179e-03, Lambda_1: 0.785, Lambda_2: 0.003800, Time: 2.81\n",
            "It: 6500, Loss: 2.077e-03, Lambda_1: 0.794, Lambda_2: 0.003808, Time: 2.81\n",
            "It: 6600, Loss: 1.980e-03, Lambda_1: 0.802, Lambda_2: 0.003815, Time: 4.04\n",
            "It: 6700, Loss: 1.885e-03, Lambda_1: 0.811, Lambda_2: 0.003822, Time: 2.91\n",
            "It: 6800, Loss: 1.794e-03, Lambda_1: 0.819, Lambda_2: 0.003828, Time: 2.78\n",
            "It: 6900, Loss: 1.704e-03, Lambda_1: 0.826, Lambda_2: 0.003834, Time: 2.86\n",
            "It: 7000, Loss: 1.617e-03, Lambda_1: 0.834, Lambda_2: 0.003839, Time: 3.36\n",
            "It: 7100, Loss: 3.329e-03, Lambda_1: 0.841, Lambda_2: 0.003843, Time: 3.49\n",
            "It: 7200, Loss: 1.455e-03, Lambda_1: 0.847, Lambda_2: 0.003847, Time: 2.80\n",
            "It: 7300, Loss: 1.476e-03, Lambda_1: 0.853, Lambda_2: 0.003850, Time: 2.86\n",
            "It: 7400, Loss: 1.569e-03, Lambda_1: 0.858, Lambda_2: 0.003853, Time: 2.83\n",
            "It: 7500, Loss: 1.411e-03, Lambda_1: 0.863, Lambda_2: 0.003856, Time: 4.04\n",
            "It: 7600, Loss: 1.376e-03, Lambda_1: 0.868, Lambda_2: 0.003858, Time: 2.82\n",
            "It: 7700, Loss: 2.502e-02, Lambda_1: 0.797, Lambda_2: 0.003846, Time: 2.83\n",
            "It: 7800, Loss: 5.693e-03, Lambda_1: 0.746, Lambda_2: 0.003954, Time: 2.80\n",
            "It: 7900, Loss: 3.317e-03, Lambda_1: 0.744, Lambda_2: 0.003986, Time: 3.48\n",
            "It: 8000, Loss: 2.570e-03, Lambda_1: 0.752, Lambda_2: 0.003998, Time: 3.43\n",
            "It: 8100, Loss: 2.275e-03, Lambda_1: 0.762, Lambda_2: 0.004004, Time: 2.82\n",
            "It: 8200, Loss: 2.106e-03, Lambda_1: 0.772, Lambda_2: 0.004008, Time: 2.82\n",
            "It: 8300, Loss: 1.983e-03, Lambda_1: 0.781, Lambda_2: 0.004013, Time: 2.80\n",
            "It: 8400, Loss: 1.882e-03, Lambda_1: 0.789, Lambda_2: 0.004018, Time: 4.12\n",
            "It: 8500, Loss: 1.792e-03, Lambda_1: 0.797, Lambda_2: 0.004024, Time: 2.83\n",
            "It: 8600, Loss: 1.710e-03, Lambda_1: 0.804, Lambda_2: 0.004030, Time: 2.82\n",
            "It: 8700, Loss: 1.634e-03, Lambda_1: 0.811, Lambda_2: 0.004036, Time: 2.80\n",
            "It: 8800, Loss: 1.560e-03, Lambda_1: 0.818, Lambda_2: 0.004042, Time: 3.51\n",
            "It: 8900, Loss: 1.490e-03, Lambda_1: 0.824, Lambda_2: 0.004048, Time: 3.43\n",
            "It: 9000, Loss: 1.422e-03, Lambda_1: 0.830, Lambda_2: 0.004054, Time: 2.82\n",
            "It: 9100, Loss: 1.356e-03, Lambda_1: 0.837, Lambda_2: 0.004059, Time: 2.82\n",
            "It: 9200, Loss: 1.293e-03, Lambda_1: 0.843, Lambda_2: 0.004065, Time: 2.95\n",
            "It: 9300, Loss: 1.234e-03, Lambda_1: 0.849, Lambda_2: 0.004070, Time: 4.04\n",
            "It: 9400, Loss: 1.177e-03, Lambda_1: 0.855, Lambda_2: 0.004075, Time: 2.83\n",
            "It: 9500, Loss: 1.124e-03, Lambda_1: 0.860, Lambda_2: 0.004079, Time: 2.88\n",
            "It: 9600, Loss: 1.073e-03, Lambda_1: 0.866, Lambda_2: 0.004082, Time: 2.81\n",
            "It: 9700, Loss: 1.025e-03, Lambda_1: 0.871, Lambda_2: 0.004086, Time: 3.56\n",
            "It: 9800, Loss: 9.789e-04, Lambda_1: 0.877, Lambda_2: 0.004088, Time: 3.39\n",
            "It: 9900, Loss: 9.402e-04, Lambda_1: 0.882, Lambda_2: 0.004090, Time: 2.79\n",
            "Starting LBFGS optimization...\n",
            "LBFGS Iteration 0, Loss: 0.000895, lambda_1: 0.886548, lambda_2: 0.004092\n",
            "LBFGS Iteration 100, Loss: 0.000302, lambda_1: 0.977023, lambda_2: 0.004140\n",
            "LBFGS Iteration 200, Loss: 0.000191, lambda_1: 0.979672, lambda_2: 0.003982\n",
            "LBFGS Iteration 300, Loss: 0.000137, lambda_1: 0.986402, lambda_2: 0.003760\n",
            "LBFGS Iteration 400, Loss: 0.000102, lambda_1: 0.993038, lambda_2: 0.003520\n",
            "LBFGS Iteration 500, Loss: 0.000078, lambda_1: 0.993621, lambda_2: 0.003372\n",
            "LBFGS Iteration 600, Loss: 0.000069, lambda_1: 0.995018, lambda_2: 0.003310\n",
            "Error lambda_1: 0.343031%\n",
            "Error lambda_2: 2.188566%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.exp(-0.438876), np.log(0.011676)"
      ],
      "metadata": {
        "id": "CuZIWlQzn7cw",
        "outputId": "c5742e79-c3fc-44f4-cb84-7061544aa337",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6447607250029861, -4.450219825990269)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SbFfSP8Kt0Vz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}